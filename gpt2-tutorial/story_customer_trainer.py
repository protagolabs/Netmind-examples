# -*- coding: utf-8 -*-
"""story_customer_trainer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r1WTCnvZ81du3b9WJy-1AkmFAtUOFm6J

## Introduction:


TODO:


## Some infomation about this task:

About the task which we will show here is story generation.

1. Story generation: We will use the GPT-2 to train a model which can generate some stories.
2. Dataset: In huggingface "KATANABRAVE/stories"
3. [GPT model](https://huggingface.co/docs/transformers/v4.32.0/en/model_doc/gpt2#transformers.GPT2Model), we will use the model via huggingface.

Before run this notebook, please ensure that these packages you have already installed.

Python version: 3.8

Packages:
numpy pandas torch torchvision torch-optimizer tqdm matplotlib sentencepiece tensorboard

Install Netmind-Mixin-Runtime:
```bash
pip install numpy pandas torch torchvision torch-optimizer tqdm matplotlib sentencepiece tensorboard
git clone https://github.com/protagolabs/NetMind-Mixin-Runtime.git
cd NetMind-Mixin-Runtime && pip install ./
pip install accelerate -U
```
"""

# Install all the package which we need, if we want to run this in the Power-codelab
"""## Not-Netmid-Part

### Step 1: Load the model and tokenizer


"""

from transformers import GPT2Tokenizer, AutoModelForCausalLM

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.add_special_tokens({'pad_token': '[PAD]'})
model = AutoModelForCausalLM.from_pretrained("gpt2")

"""### Step 2: Prepare the dataset.

"""

from datasets import load_dataset
from torch.utils.data import DataLoader

# Import the dataset, which is a demo for some D&D stories.
stories = load_dataset("KATANABRAVE/stories")

train_data = stories["train"]
eval_data = stories["validation"]

train_dataloader = DataLoader(train_data, batch_size=4, shuffle=False)

"""### Step 3: Define the training parameters

"""

# Customer Trainer
import torch
from tqdm import tqdm
import transformers
from transformers import TrainingArguments, Trainer
from torch.nn.utils import clip_grad_norm_
from transformers import get_linear_schedule_with_warmup

training_args = TrainingArguments(
        evaluation_strategy = "epoch",
        learning_rate=2e-4,
        max_steps = 1000,
        save_steps = 100,
        weight_decay=0.01,
        per_device_train_batch_size=4,
        do_eval = False,
        do_train = True,
        num_train_epochs = 5,
        output_dir = "./test"
    )

"""### Step 4: Define the customer trainer, note that we need insert step_callback for monitoring training loss."""

def train(dataset, training_args, model, optimizer, scheduler, step_callback):

    schedule_total = training_args.max_steps

    train_data = dataset

    device = torch.device("cuda:{}".format(training_args.local_rank))
    completed_steps = 0
    for epoch in range(training_args.num_train_epochs):
        progress_bar = tqdm(range( training_args.max_steps ))
        progress_bar.set_description(f'**Epoch: {epoch}**')

        model.train()
        total_loss = 0

        for train_step, batch in enumerate(train_data):
            optimizer.zero_grad()

            input_ids = torch.tensor([ids.tolist() for ids in batch['input_ids']])
            attention_mask = torch.tensor([ids.tolist() for ids in batch['attention_mask']])
            labels = torch.tensor([ids.tolist() for ids in batch['labels']])

            input_ids = input_ids.to(device, non_blocking=True)
            attention_mask = attention_mask.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)

            outputs = model(input_ids=input_ids,
                        attention_mask=attention_mask,
                        labels=labels)
            loss = outputs.loss
            # We keep track of the loss at each epoch

            total_loss += loss.detach().float()
            # loss = loss / self.gradient_accumulation_steps
            # accelerator.backward(loss)
            loss.backward()
            if training_args.max_grad_norm > 0:
                clip_grad_norm_(model.parameters(), training_args.max_grad_norm)
            optimizer.step()
            scheduler.step()
            # average loss in one epoch
            loss2log = total_loss.item()/ (train_step+1)
            lr2log  = scheduler.get_last_lr()[0]
            progress_bar.set_postfix(loss=loss2log , lr=lr2log )
            progress_bar.update(1)
            completed_steps += 1

            monitor_metrics = {
                "loss": loss.item(),
                "Learning rate": scheduler.get_last_lr()[0]
            }

            step_callback(monitor_metrics)

            if completed_steps == training_args.max_steps:
                return

    # Just for nividia-smi visiable memory release
    torch.cuda.empty_cache()

import torch

device = torch.device("cuda:{}".format(training_args.local_rank))
model.to(device)

"""## Netmind-Part

### Step 5: Initialize the Netmind nmp

The Enviroments' parameters mush be set before import nmp

- load_checkpoint (Default True): Whether to load checkpoint saved in previous training. It only works when training on the Netmind platform.
- use_ddp (Default False): Whether to use ddp in current training.  

Initialize the data of the "Mixin" library. This function should be placed before the actual start of training, and  before other "Mixin" API calls
"""

from NetmindMixins.Netmind import nmp, NetmindOptimizer, NetmindDistributedModel

nmp.init(load_checkpoint=False, use_ddp=True)

"""### Step 6: Set the model to NetmindDistributedModel

- model: Model variable.
  
Wrap machine learning model with "NetmindDistributedModel". This will not change the ML model itself. It can be placed anywhere after the "model" is defined and before the actual start of training.
"""

"""
USER MODIFY: Can change the mode type.For example: just set `ddp_model = NetmindDistributedModel(mdoel)` which means
do not use the ddp
"""
ddp_model = NetmindDistributedModel(
    torch.nn.parallel.DistributedDataParallel(model, device_ids=[training_args.local_rank], output_device=training_args.local_rank))

"""### Step 7: Define the optimizer, and set it be the NetmindOptimizer

- optimizer: optimizer variable.

Wrap machine learning model optimizer with "NetmindOptimizer". This will not change the optimizer itself. It can be placed anywhere after the "optimizer" is defined and before the actual start of training.
"""

"""
USER MODIFY: The optimize should suit the model. And use the NetmindOptimizer
"""
from transformers import AdamW
from transformers import get_linear_schedule_with_warmup
# setup optimizer...
model.train()

param_optimizer = list(model.named_parameters())
no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
optimizer_grouped_parameters = [
    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': training_args.weight_decay},
    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]
optimizer = AdamW(optimizer_grouped_parameters, lr=training_args.learning_rate)
optimizer = NetmindOptimizer(optimizer)
schedule_total = training_args.max_steps

scheduler = get_linear_schedule_with_warmup(
    optimizer, num_warmup_steps=training_args.warmup_steps, num_training_steps=schedule_total
)

"""### Setp 8: Start Training"""

"""
USER MODIFY: set the process bar. And start the train.
"""

nmp.init_train_bar(total_epoch=training_args.num_train_epochs, step_per_epoch=len(stories["train"]))
train(train_dataloader, training_args, model, optimizer, scheduler, nmp.step)
nmp.finish_training() # Finish the training. It should be placed at the end of file